/*
 * DBSCAN.hpp
 * 
 * Copyright 2017 Andreas Recke <andreas@AntigoneLinux>
 * 
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 * 
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,
 * MA 02110-1301, USA.
 * 
 * 
 */
 
#include <iostream>
#include <cassert>

#include "../cpp/BenchmarkSingleton.hpp"

#include <thrust/device_vector.h>
#include <thrust/functional.h>
#include <thrust/execution_policy.h>

#include <thrust/sequence.h>
#include <thrust/scatter.h>
#include <thrust/gather.h>

#include <thrust/fill.h>
#include <thrust/unique.h>
#include <thrust/copy.h> 
#include <thrust/reduce.h> 
#include <thrust/transform.h> 
#include <thrust/replace.h> 
#include <thrust/count.h> 
#include <thrust/iterator/transform_iterator.h>
#include <thrust/iterator/permutation_iterator.h>
#include <thrust/iterator/counting_iterator.h>
#include <thrust/iterator/constant_iterator.h>
#include <thrust/iterator/zip_iterator.h>
#include <thrust/inner_product.h>
#include "SpecialFunctors.hcu"
#include "SpecialIterators.hcu"
 
 
//*** some kernels for this method ************************************************************************

//*********************************************************************************************************
// this device function looks for the root of an entry and sets it to a new position
/* halving */
template <typename root_iterator_type>
__device__
void set_new_root(root_iterator_type roots_iter, unsigned int entry, unsigned int new_root)
{
	using root_value_type = typename root_iterator_type::value_type;
	static_assert(std::is_same<root_value_type, unsigned int>::value, "Root iterator has not value type unsigned int");
	
	unsigned int cur;
	unsigned int next = entry;
	do 
	{
		cur = next;
		next = atomicExch( thrust::raw_pointer_cast(& (* (roots_iter + cur))), new_root );
	} while (next != cur);
	
}

template <typename root_iterator_type>
__device__
void union_a_and_b(root_iterator_type roots_iter, unsigned int point_a, unsigned int point_b)
{
	using root_value_type = typename root_iterator_type::value_type;
	static_assert(std::is_same<root_value_type, unsigned int>::value, "Root iterator has not value type unsigned int");
	
	unsigned int cur, next, new_root;
	
	next     = (point_a > point_b)? point_a : point_b;
	new_root = (point_a > point_b)? point_b : point_a;
	
	/*
	 // not useful
	cur = *(roots_iter + new_root);
	while (cur != new_root)
	{
		cur = new_root;
		new_root = *(roots_iter + new_root);
	}
	*/
	
	bool success;
	do	
	{
		cur = next;
		next = atomicExch( thrust::raw_pointer_cast(& (* (roots_iter + cur))), new_root );
			success = (cur == next) || (next == new_root);	
		if (new_root > next) thrust::swap(next, new_root);
	} 
	while (!success);	
}


//*********************************************************************************************************
// this device function only changes a root if it hasn't been changed before

template <typename root_iterator_type>
__device__
void set_new_root_only_if_not_defined_before(root_iterator_type roots_iter, unsigned int entry, unsigned int new_root)
{
	using root_value_type = typename root_iterator_type::value_type;
	
	root_value_type *ptr = thrust::raw_pointer_cast(& (* (roots_iter + entry)));
	
	atomicCAS(ptr, entry, new_root);
}


//*********************************************************************************************************
// this simple kernel just follows the branches of a tree at a each position to its root and saves this information


template <typename root_iterator_type>
__global__
void 
tree_reduction_kernel(root_iterator_type roots_iter, unsigned int n_points)
{
	using root_value_type = typename root_iterator_type::value_type;
	
	for (root_value_type x = blockIdx.x * blockDim.x + threadIdx.x; x < n_points; x+= blockDim.x*gridDim.x)
	{
		root_value_type cur  = x;
		root_value_type next = *(roots_iter + cur);
		
		while (next != cur)
		{
			cur = next;
			next = *(roots_iter + cur);
		}
		
		atomicExch(thrust::raw_pointer_cast(&(*(roots_iter + x))), cur);
	}
}

//*********************************************************************************************************
// this kernel is for evaluation of this algorithm only.

template <typename root_iterator_type, typename depth_iterator_type>
__global__
void 
tree_depth_evaluation_kernel(root_iterator_type roots_iter, depth_iterator_type depth_iterator, unsigned int n_points, unsigned int max_depth)
{
	using root_value_type = typename root_iterator_type::value_type;
	using depth_value_type = typename depth_iterator_type::value_type;
	
	for (root_value_type x = blockIdx.x * blockDim.x + threadIdx.x; x < n_points; x+= blockDim.x*gridDim.x)
	{
		depth_value_type depth = 0;
		root_value_type cur  = x;
		root_value_type next = *(roots_iter + cur);
		
		while (next != cur && depth < max_depth)
		{
			++depth;
			cur = next;
			next = *(roots_iter + cur);
		}
		
		atomicAdd(thrust::raw_pointer_cast(&(*(depth_iterator + depth))), 1);
	}
}


//****************************************************************************************** 
// These functions were prototyped in R and are necessary to calculate positions in
// the scanning diagonals



template <typename str_index_iterator_type>
__device__
inline void strided_copy_global_to_shared_string(str_index_iterator_type str_index_iterator, char* shared_pointer)
{
	unsigned int len = thrust::get<1>(*str_index_iterator);
	char* raw_string_pointer = thrust::raw_pointer_cast( & (thrust::get<0>(*str_index_iterator) ));
	
	for (unsigned int i=threadIdx.x; i < len; i+= blockDim.x)
	{
		shared_pointer[i] = raw_string_pointer[i]; 
	}
}


__device__
int
diag_to_i(int D, int index, int width)
{
  return index - (width/2) + (D/2);
}

__device__
int
diag_to_j(int D, int index, int width)
{
  return ((D+1)/2) - index + (width/2);
}

__device__
int
ij_to_diag(int i, int j)
{
  return i+j;
}

__device__
int
ij_to_index(int i, int j, int width)
{
  return i - ((i+j)/2) + (width/2);
}

__device__
int
ij_to_address(int i, int j, int width)
{
  int dx = ij_to_diag(i,j) % 2;
  int tx = ij_to_index(i,j, width);
  return dx*width + tx;
}

__device__
int
diag_to_address(int D, int index, int width)
{
  return (D % 2)*width + index;
}


//****************************************************************************************** 
__device__
int
improved_levenshtein_distance(char* subjstring, unsigned int subj_len, char* querystring, unsigned int query_len, int* shared_memory_block)
{
	int width = 2 + max(query_len, subj_len);
	
	for (int n = threadIdx.x; n < 2*width; n+=blockDim.x) shared_memory_block[n] = 0;
	
	for (int dx = 0; dx < query_len + subj_len + 1; ++dx)
    {
		int initval = (dx == 0)? 0 : dx;
        
	    for (int index = threadIdx.x; index <  width; index += blockDim.x)
	    {
			int ix = diag_to_i(dx, index, width);
			int jx = diag_to_j(dx, index, width);
            
			if (ix >= 0 && ix <= subj_len && jx >= 0 && jx <= query_len)
	        {
				int ma = diag_to_address(dx, index, width);
				if (ix == 0 || jx == 0)
				{
					shared_memory_block[ma] = initval;
				} 
				else 
				{
					int ma11 = ij_to_address(ix-1, jx-1, width);
					int ma01 = ij_to_address(ix, jx-1, width);
					int ma10 = ij_to_address(ix-1, jx, width);
          				
					int val11 = shared_memory_block[ma11];
					if (subjstring[ix-1] != querystring[jx-1]) val11 += 1;
          
					int val01 = shared_memory_block[ma01] + 1;
					int val10 = shared_memory_block[ma10] + 1;
          
					shared_memory_block[ma] = min(min(val11, val01), val10);
				}
			}
		}
		__syncthreads();
	}
  
	return shared_memory_block[ij_to_address(subj_len, query_len, width)];  
}

//****************************************************************************************** 
// this is a variation of the above function. It is able to cope with a 2D kernel.
// The problem is the syncing of threads, because the stringlengths are different.
// We therefore use a standard maximum length "max_strlen"

__device__
int
improved_levenshtein_distance(char* subjstring, unsigned int subj_len, char* querystring, unsigned int query_len, unsigned int max_strlen, int* shared_memory_block)
{
	int width = 2 + max_strlen;
	int result;
	
	for (int n = threadIdx.x; n < 2*width; n+=blockDim.x) shared_memory_block[n] = 0;
	
	//for (int dx = 0; dx < query_len + subj_len + 1; ++dx)
	for (int dx = 0; dx < 2*max_strlen + 1; ++dx)
    {
		//???????????????????
		
		int initval = (dx == 0)? 0 : dx;
        
	    for (int index = threadIdx.x; index <  width; index += blockDim.x)
	    {
			int ix = diag_to_i(dx, index, width);
			int jx = diag_to_j(dx, index, width);
            
			if (ix >= 0 && ix <= subj_len && jx >= 0 && jx <= query_len)
	        {
				int ma = diag_to_address(dx, index, width);
				if (ix == 0 || jx == 0)
				{
					shared_memory_block[ma] = initval;
				} 
				else 
				{
					int ma11 = ij_to_address(ix-1, jx-1, width);
					int ma01 = ij_to_address(ix, jx-1, width);
					int ma10 = ij_to_address(ix-1, jx, width);
          				
					int val11 = shared_memory_block[ma11];
					if (subjstring[ix-1] != querystring[jx-1]) val11 += 1;
          
					int val01 = shared_memory_block[ma01] + 1;
					int val10 = shared_memory_block[ma10] + 1;
          
					shared_memory_block[ma] = min(min(val11, val01), val10);
				}
			}
		}
		__syncthreads();
	}
  
	return shared_memory_block[ij_to_address(subj_len, query_len, width)];  
}



 
//****************************************************************************************** 
// this device function expects a 2nd dimension of a block and 
// uses _syncthreads carefully! ThreadIdx.y contains the index of the string in shared memory

__device__
int
improved_2D_levenshtein_distance(char* subjstring, unsigned int subj_len, char* querystring, unsigned int query_len, int* shared_memory_block, unsigned int max_strlen)
{
	int width = 2 + max(subj_len, query_len);
	
	unsigned int query_str_index = max_strlen * threadIdx.y;
	unsigned int mem_index = 2*(max_strlen+2) * threadIdx.y;
	
	int active_threads = 1;	
	
	for (unsigned int n = threadIdx.x; n < 2*width; n+=blockDim.x) shared_memory_block[mem_index + n] = 0;
	
	int dx = 0;
	while (active_threads)
	{
		int initval = (dx == 0)? 0 : dx;
        
	    for (int index = threadIdx.x; index <  width; index += blockDim.x)
	    {
			int ix = diag_to_i(dx, index, width);
			int jx = diag_to_j(dx, index, width);
            
			if (ix >= 0 && ix <= subj_len && jx >= 0 && jx <= query_len)
	        {
				int ma = diag_to_address(dx, index, width);
				if (ix == 0 || jx == 0)
				{
					shared_memory_block[mem_index + ma] = initval;
				} 
				else 
				{
					int ma11 = ij_to_address(ix-1, jx-1, width);
					int ma01 = ij_to_address(ix, jx-1, width);
					int ma10 = ij_to_address(ix-1, jx, width);
          				
					int val11 = shared_memory_block[mem_index + ma11];
					if (subjstring[ix-1] != querystring[query_str_index+jx-1]) val11 += 1;
          
					int val01 = shared_memory_block[mem_index + ma01] + 1;
					int val10 = shared_memory_block[mem_index + ma10] + 1;
          
					shared_memory_block[mem_index + ma] = min(min(val11, val01), val10);
				}
			}
		}
		++dx;
		active_threads = __syncthreads_count(dx < query_len + subj_len + 1) > 0;
	}
  
	return shared_memory_block[mem_index + ij_to_address(subj_len, query_len, width)];  
}


//****************************************************************************************** 

__device__
int
improved_hamming_distance(char* subjstring, unsigned int subj_len, char* querystring, unsigned int query_len)
{
	int neqsum = 0;
	int sync_secure = 0;			
				
	for (unsigned int tx = threadIdx.x; sync_secure * blockDim.x < query_len; tx += blockDim.x, ++sync_secure)
	{	
		int neq;		
		if (tx < query_len) 
			neq =  querystring[tx] != subjstring[tx];
		else
			neq = 0;
							
		neqsum += __syncthreads_count(neq);
	}
	return neqsum;
}


//*********************************************************************************************************
// these kernels do an exact comparison of bitsets

// Version 1
template <int sN, class iterator_tuple_type, typename max_width_tuple_type> 
__device__ 
int bitset_values_equal (iterator_tuple_type bitvalues1, iterator_tuple_type bitvalues2, max_width_tuple_type gene_width_tuple, Int2Type<sN>)
{
	unsigned int current_width = thrust::get<sN>(gene_width_tuple);
	
	typedef Int2Type<sN - 1> mapType;
	mapType mapInt;
	
	int cmp = 1;
	int result = compare_bitset_values(bitvalues1, bitvalues2, gene_width_tuple, mapInt);
	
	int *pointer1 = thrust::raw_pointer_cast( & (thrust::get<sN>(*bitvalues1)) );
	int *pointer2 = thrust::raw_pointer_cast( & (thrust::get<sN>(*bitvalues2)) );
	
	for (int position = threadIdx.x; position < current_width; position += blockDim.x)
	{
		cmp = cmp && (pointer1[position] == pointer2[position]);
	}
	
	result = result && (__syncthreads_count(cmp) > 0);
	
	return result;
}	
	
// Version 2	
template <class iterator_tuple_type, typename max_width_tuple_type> 
__device__ 
int bitset_values_equal(iterator_tuple_type bitvalues1, iterator_tuple_type bitvalues2, max_width_tuple_type gene_width_tuple, Int2Type<0>)
{
	unsigned int current_width = thrust::get<0>(gene_width_tuple);
	
	int *pointer1 = thrust::raw_pointer_cast( & (thrust::get<0>(*bitvalues1)) );
	int *pointer2 = thrust::raw_pointer_cast( & (thrust::get<0>(*bitvalues2)) );
	
	int cmp = 1;
	for (int position = threadIdx.x; position < current_width; position += blockDim.x)
	{
		cmp = cmp && (pointer1[position] == pointer2[position]);
	}
	
	return (__syncthreads_count(cmp) > 0);
}	


// BaseCaller
template <class iterator_tuple_type, typename max_width_tuple_type> 
__device__ 
int bitset_values_equal(iterator_tuple_type bitvalues1, iterator_tuple_type bitvalues2, max_width_tuple_type gene_width_tuple)
{
	typedef Int2Type<thrust::tuple_size<typename iterator_tuple_type::value_type>::value-1> mapType;
	mapType map;
	
	return bitset_values_equal(bitvalues1, bitvalues2, gene_width_tuple, map);
}

//**********************************************************************************************************





//**********

template <typename bitset_iterator_type, typename bitset_properties_type, typename result_iterator_type>
__global__
void
find_eq_vgenes(bitset_iterator_type bitset_iterator_1, 
			   bitset_iterator_type bitset_iterator_2, 
			   bitset_properties_type bitset_properties, 
			   result_iterator_type result_iter)
{
	if (singlet_bitset_values_equal(bitset_iterator_1, bitset_iterator_2 + blockIdx.x, bitset_properties)) 
	{ 
		*(result_iter + blockIdx.x) = 1;
		int ij[4], jj[4];
		int *pp1 = thrust::raw_pointer_cast(& thrust::get<0>(*bitset_iterator_1));
		int *pp2 = thrust::raw_pointer_cast(& thrust::get<0>(*(bitset_iterator_2 + blockIdx.x)));
		for (unsigned int x = 0; x < 4; ++x) { ij[x] = pp2[x]; jj[x] = pp1[x]; }
		printf("( %d %d %d %d = %d %d %d %d )", jj[0], jj[1], jj[2], jj[3], ij[0], ij[1], ij[2], ij[3]);
	}
}
	


//*********************************************************************************************************
// this kernel does an exact match comparison
// it contains a lot of branches, which are actually on block level and not thread level. So this should
// not reduce performance too much

template <typename bitset_iterator_type, typename bitset_properties_type, typename stringset_iterator_type, typename root_iterator_type>
__global__
void 
singlet_find_duplicates_kernel(bitset_iterator_type bitset_iterator, 
					   bitset_properties_type bitset_properties, 
                       stringset_iterator_type stringset_iterator, 
                       root_iterator_type roots_iterator, 
                       unsigned n_points)
{	
	// first, do the grid stride loop
	for (unsigned int x_index = blockIdx.x*blockDim.x + threadIdx.x; x_index < n_points; x_index += blockDim.x * gridDim.x)
	{
		for (unsigned int y_index = blockIdx.y*blockDim.y + threadIdx.y; y_index < n_points; y_index += blockDim.y * gridDim.y)
		{
			unsigned int query_index = x_index; //  + threadIdx.y * blockDim.x; query_index = query_index % n_points;
			unsigned int subj_index  = y_index; //  + threadIdx.x * blockDim.y; subj_index  = subj_index  % n_points;
			// now we're in business
			if (query_index < subj_index)  // avoid duplicate comparisons and comparisons with itself
			{
				if (thrust::get<2>(*(stringset_iterator + query_index)) == thrust::get<2>(*(stringset_iterator + subj_index)))  // if string hashes are identical
				{
					unsigned int query_len = thrust::get<1>(*(stringset_iterator + query_index));
					unsigned int subj_len  = thrust::get<1>(*(stringset_iterator + subj_index));
					if (query_len == subj_len) // and if string lengths are identical
					{
						
						if (singlet_bitset_values_equal(bitset_iterator + query_index, bitset_iterator + subj_index, bitset_properties))
						{
							char *query_string_ptr = thrust::raw_pointer_cast(& thrust::get<0>(*(stringset_iterator + query_index)) );
							char  *subj_string_ptr = thrust::raw_pointer_cast(& thrust::get<0>(*(stringset_iterator + subj_index)) );
							unsigned int position = 0;
							bool eq = true;
							while (position < query_len && eq)
							{
								eq = query_string_ptr[position] == subj_string_ptr[position];
								++position;
							}
							if (eq)
							{
								// set_new_root(roots_iterator, subj_index, query_index);
								union_a_and_b(roots_iterator, subj_index, query_index);
							}		
						}						
					}
				} 
			} 
		} 
	}
}





//*********************************************************************************************************
//*********************************************************************************************************

// these two kernels are for a pure Hamming distance approach


template <typename bitset_iterator_type, typename bitset_properties_type, typename stringset_iterator_type,
		  typename unique_indices_iterator_type, typename copies_iterator_type,
		  typename core_point_iterator_type, typename distance_functor_type>
__global__
void 
half_hamming_core_points( bitset_iterator_type bitset_iterator, 
				  bitset_properties_type bitset_properties, 
				  stringset_iterator_type stringset_iterator,
				  unique_indices_iterator_type unique_indices_iterator,
				  copies_iterator_type copies_iterator,
				  core_point_iterator_type core_point_iterator,			      
				  unsigned int n_points,
				  unsigned int max_strlen,
				  distance_functor_type distance_functor,
				  unsigned int minPts)
{
	for (unsigned int x_preindex = blockIdx.x*blockDim.x + threadIdx.x; x_preindex < n_points; x_preindex += blockDim.x * gridDim.x)
	{
		for (unsigned int y_preindex = blockIdx.y*blockDim.y + threadIdx.y; y_preindex < n_points; y_preindex += blockDim.y * gridDim.y)
		{
			unsigned int x_index = *(unique_indices_iterator + x_preindex);
			unsigned int y_index = *(unique_indices_iterator + y_preindex);
			
			if (x_index < y_index && ( *(core_point_iterator + x_index) < minPts  || *(core_point_iterator + y_index) < minPts ))
			{				
				int x_len = thrust::get<1>(*(stringset_iterator + x_index)),
					y_len = thrust::get<1>(*(stringset_iterator + y_index));
						 
				int schwelle = distance_functor(x_len, y_len);
			
				if (x_len == y_len)
				{
					
					if (singlet_compare_bitset_values(bitset_iterator + x_index, bitset_iterator + y_index, bitset_properties))
					{
						char *x_string_ptr = thrust::raw_pointer_cast(& thrust::get<0>(*(stringset_iterator + x_index)) );
						char *y_string_ptr = thrust::raw_pointer_cast(& thrust::get<0>(*(stringset_iterator + y_index)) );
							
						int hamming_dist = 0;
					
						bool result = true;
						unsigned int position = 0; 						
						
						while (position < x_len && result)
						{
							if (x_string_ptr[position] != y_string_ptr[position]) 
							{
								++hamming_dist;
								result = hamming_dist <= schwelle;
							}
							++position;
						}
							
						if (result)
						{
							static_assert(std::is_same<typename core_point_iterator_type::value_type, typename copies_iterator_type::value_type>::value, "Core and copies iterator value types are not identical!");
					
							typename core_point_iterator_type::value_type *core_at_x = thrust::raw_pointer_cast(&  (*(core_point_iterator + x_index)));
							typename core_point_iterator_type::value_type *core_at_y = thrust::raw_pointer_cast(&  (*(core_point_iterator + y_index)));
					
							typename copies_iterator_type::value_type x_copies = *(copies_iterator + x_index);
							typename copies_iterator_type::value_type y_copies = *(copies_iterator + y_index);
							
							atomicAdd(core_at_x, y_copies);
							
							atomicAdd(core_at_y, x_copies);					
						} // if (result)
					}
				}	
			}			
		} // for 
	} // for	
}


//*********************************************************************************************************
//*********************************************************************************************************
// this kernel is fuses the data in a purely hamming distance approach

template <typename bitset_iterator_type, typename bitset_properties_type, typename stringset_iterator_type, typename roots_iterator_type, 
		  typename core_point_indices_iterator_type,
          typename unique_indices_iterator_type, typename core_point_iterator_type, typename distance_functor_type>
__global__
void 
fuse_hamming_clusters( bitset_iterator_type bitset_iterator, 
						bitset_properties_type bitset_properties, 
					    stringset_iterator_type stringset_iterator,
					    roots_iterator_type roots_iterator,
					    core_point_iterator_type core_point_iterator,
					    core_point_indices_iterator_type core_point_indices_iterator,
					    unique_indices_iterator_type unique_indices_iterator,
					    unsigned int n_cores,
					    unsigned int n_uniques,
					    unsigned int max_strlen,
					    distance_functor_type distance_functor)
{
	// were working with singlet kernels
	
	for (unsigned int x_preindex = blockIdx.x*blockDim.x + threadIdx.x; x_preindex < n_cores; x_preindex += blockDim.x * gridDim.x)
	{
		for (unsigned int y_preindex = blockIdx.y*blockDim.y + threadIdx.y; y_preindex < n_uniques; y_preindex += blockDim.y * gridDim.y)
		{
			unsigned int x_index = *(core_point_indices_iterator + x_preindex);
			unsigned int y_index = *(unique_indices_iterator + y_preindex);
			
			if (x_index < y_index ||  !(*(core_point_iterator + y_index)))
			{
				int x_len = thrust::get<1>(*(stringset_iterator + x_index)),
					y_len = thrust::get<1>(*(stringset_iterator + y_index));
						 
				int schwelle = distance_functor(x_len, y_len);
												
				if (x_len == y_len)
				{
					
					if (singlet_compare_bitset_values(bitset_iterator + x_index, bitset_iterator + y_index, bitset_properties))
					{
						char *x_string_ptr = thrust::raw_pointer_cast(& thrust::get<0>(*(stringset_iterator + x_index)) );
						char *y_string_ptr = thrust::raw_pointer_cast(& thrust::get<0>(*(stringset_iterator + y_index)) );
							
						int hamming_dist = 0;
					
						bool result = true;
						unsigned int position = 0;
						
						while (position < x_len && result)
						{
							if (x_string_ptr[position] != y_string_ptr[position]) ++hamming_dist;
							result = hamming_dist <= schwelle;
							++position;
						}
							
						if (result)
						{
							// x will be the master
							unsigned int x_root = x_index; // find_root(roots_iterator, x_index);
							
							// y will be the slave
							int is_y_core = *(core_point_iterator + y_index);
							
							if (is_y_core)
							{
								// find root of y and attach it to x_root
								// set_new_root(roots_iterator, y_index, x_root);
								union_a_and_b(roots_iterator, y_index, x_root);
							}
							else // y is not a core point and will be bound only once!
							{
								set_new_root_only_if_not_defined_before(roots_iterator, y_index, x_root);
							}
							
						} // if (result)
					}
				}					
			} // if (x_index < y_index)
		} // for 
	} // for	
}


//*********************************************************************************************************
//*********************************************************************************************************
// This kernel is an alternate version to identify core points



template <typename bitset_iterator_type, typename bitset_properties_type, typename stringset_iterator_type,
		  typename unique_indices_iterator_type, typename copies_iterator_type,
		  typename core_point_iterator_type, typename distance_functor_type>
__global__
void 
half_core_points( bitset_iterator_type bitset_iterator, 
				  bitset_properties_type bitset_properties, 
				  stringset_iterator_type stringset_iterator,
				  unique_indices_iterator_type unique_indices_iterator,
				  copies_iterator_type copies_iterator,
				  core_point_iterator_type core_point_iterator,			      
				  unsigned int n_uniques,
				  unsigned int max_strlen,
				  distance_functor_type distance_functor,
				  unsigned int minPts)
{
	extern __shared__ int cs[];
	// first, do the grid stride loop
	for (unsigned int x_preindex = blockIdx.x; x_preindex < n_uniques; x_preindex += gridDim.x)
	{
		for (unsigned int y_preindex = blockIdx.y; y_preindex < n_uniques; y_preindex += gridDim.y)
		{
			unsigned int x_index = *(unique_indices_iterator + x_preindex);
			unsigned int y_index = *(unique_indices_iterator + y_preindex);
			
			if (x_index < y_index && ( *(core_point_iterator + x_index) < minPts  || *(core_point_iterator + y_index) < minPts ))
			{
				int result = 1;
		
				int x_len = thrust::get<1>(*(stringset_iterator + x_index)),
					y_len = thrust::get<1>(*(stringset_iterator + y_index));
						 
				int schwelle = distance_functor(x_len, y_len);
				
				int distance = (x_len > y_len)? (x_len - y_len) : (y_len - x_len);		
				int hamming_active = 0;
						
				result = compare_bitset_values(bitset_iterator + x_index, bitset_iterator + y_index, bitset_properties) && distance <= schwelle;
				
				if (result)
				{		
					// distribute shared memory
					char* x_string  = (char*)cs;
					char* y_string = &x_string[max_strlen];
					int* residual_shared_memory = (int*)&y_string[max_strlen];
					
					strided_copy_global_to_shared_string(stringset_iterator + x_index, x_string);
					strided_copy_global_to_shared_string(stringset_iterator + y_index, y_string);
								
					// then calculate Hamming distance
					if (x_len == y_len)
					{
						result = improved_hamming_distance(x_string, x_len, y_string, y_len) <= schwelle;
						hamming_active = 1;
					}
											
					if (!(hamming_active && result))
					{
						distance = improved_levenshtein_distance(x_string, x_len, y_string, y_len, residual_shared_memory);
						result   = distance <= schwelle;
					}
				} 
				
				if (threadIdx.x == 0 && result) 
				{
					static_assert(std::is_same<typename core_point_iterator_type::value_type, typename copies_iterator_type::value_type>::value, "Core and copies iterator value types are not identical!");
					
					typename core_point_iterator_type::value_type *core_at_x = thrust::raw_pointer_cast(&  (*(core_point_iterator + x_index)));
					typename core_point_iterator_type::value_type *core_at_y = thrust::raw_pointer_cast(&  (*(core_point_iterator + y_index)));
					
					typename copies_iterator_type::value_type x_copies = *(copies_iterator + x_index);
					typename copies_iterator_type::value_type y_copies = *(copies_iterator + y_index);
					
					atomicAdd(core_at_x, y_copies);
					
					atomicAdd(core_at_y, x_copies);						
					
				} // if (threadIdx.x == 0 && result) 			
			}
			
		} // for 
	} // for	
}



//*********************************************************************************************************
// this kernel is fuses the data

template <typename bitset_iterator_type, typename bitset_properties_type, typename stringset_iterator_type, typename roots_iterator_type, 
		  typename core_point_indices_iterator_type,
          typename unique_indices_iterator_type, typename core_point_iterator_type, typename distance_functor_type>
__global__
void 
fuse_clusters( bitset_iterator_type bitset_iterator, 
						bitset_properties_type bitset_properties, 
					    stringset_iterator_type stringset_iterator,
					    roots_iterator_type roots_iterator,
					    core_point_iterator_type core_point_iterator,
					    core_point_indices_iterator_type core_point_indices_iterator,
					    unique_indices_iterator_type unique_indices_iterator,
					    unsigned int n_cores,
					    unsigned int n_unique,
					    unsigned int max_strlen,
					    distance_functor_type distance_functor)
{
	extern __shared__ int cs[];
	// first, do the grid stride loop
	for (unsigned int x_preindex = blockIdx.x; x_preindex < n_cores; x_preindex += gridDim.x)
	{
		for (unsigned int y_preindex = blockIdx.y; y_preindex < n_unique; y_preindex += gridDim.y)
		{
			unsigned int x_index = *(core_point_indices_iterator + x_preindex);
			unsigned int y_index = *(unique_indices_iterator + y_preindex);
			
			if (x_index < y_index || !(*(core_point_iterator + y_index)))
			{
								
				// will proceed only, when x is a core point! 
					
				// now we're in business
				
				int x_len = thrust::get<1>(*(stringset_iterator + x_index)),
					y_len = thrust::get<1>(*(stringset_iterator + y_index));
						 
				int schwelle = distance_functor(x_len, y_len);
				
				int distance = (x_len > y_len)? (x_len - y_len) : (y_len - x_len);		
				int result = 1;
				int hamming_active = 0;
						
				result = compare_bitset_values(bitset_iterator + x_index, bitset_iterator + y_index, bitset_properties) && distance <= schwelle;
				
				if (result)
				{		
					// distribute shared memory
					char* x_string  = (char*)cs;
					char* y_string = &x_string[max_strlen];
					int* residual_shared_memory = (int*)&y_string[max_strlen];
					
					strided_copy_global_to_shared_string(stringset_iterator + x_index, x_string);
					strided_copy_global_to_shared_string(stringset_iterator + y_index, y_string);
								
					// then calculate Hamming distance
					if (x_len == y_len)
					{
						result = improved_hamming_distance(x_string, x_len, y_string, y_len) <= schwelle;
						hamming_active = 1;
					}
											
					if (!(hamming_active && result))
					{
						distance = improved_levenshtein_distance(x_string, x_len, y_string, y_len, residual_shared_memory);
						result   = distance <= schwelle;
					}
				} 
				
				if (threadIdx.x == 0 && result) {
					// x will be the master
					unsigned int x_root = x_index; // find_root(roots_iterator, x_index);
					
					// y will be the slave
					int is_y_core = *(core_point_iterator + y_index);
					
					if (is_y_core)
					{
						// find root of y and attach it to x_root
						// set_new_root(roots_iterator, y_index, x_root);
						union_a_and_b(roots_iterator, y_index, x_root);
					}
					else // y is not a core point and will be bound only once!
					{
						set_new_root_only_if_not_defined_before(roots_iterator, y_index, x_root);
					}
					
				} // if (threadIdx.x == 0 && result) 
			
			} // if (x_index < y_index)
		} // for 
	} // for	
}


//*********************************************************************************************************
 
 
template <class DataManagementClass>
class DBSCAN
{
	
	DataManagementClass& DataManager;
	
	thrust::device_vector<unsigned int> roots;
	thrust::device_vector<int> is_core_point_vector;
	thrust::device_vector<unsigned int> uniques, cluster_ids;
	thrust::device_vector<int> copies;
	
	unsigned int max_blocks[3];
	unsigned int max_threads;
	unsigned int max_threads_dim[3];
	unsigned int max_shared_memory_per_block;
	unsigned int max_threads_by_multiprocessor;
	
	unsigned int minPts;
	float threshold;
	unsigned int n_unique;
	bool pure_hamming_approach;
	bool evaluate_trees_wished;
		
	public:

	DBSCAN(DataManagementClass& _DataManager, float _threshold, unsigned int _minPts) : 
		DataManager(_DataManager),
		roots(_DataManager.get_n_entries()),
		uniques(_DataManager.get_n_entries()),
		copies(_DataManager.get_n_entries(),1),
		cluster_ids(_DataManager.get_n_entries(),1),
		is_core_point_vector(_DataManager.get_n_entries()),
		pure_hamming_approach(false),
		evaluate_trees_wished(false)
	{
		threshold = _threshold;
		minPts = _minPts;
		
		// get some device properties
		cudaDeviceProp devProp;
        cudaGetDeviceProperties(&devProp, 0);
        max_shared_memory_per_block = devProp.sharedMemPerBlock;
		max_threads = devProp.maxThreadsPerBlock;
		max_threads_by_multiprocessor = devProp.maxThreadsPerMultiProcessor;
		for (unsigned int i=0; i < 3; ++i) {
			max_blocks[i]      = devProp.maxGridSize[i];
			max_threads_dim[i] = devProp.maxThreadsDim[i];
		}
		
		BenchmarkSingleton::Instance().Message("DBSCAN settings: epsilon = " + std::to_string(threshold));
		BenchmarkSingleton::Instance().Message("DBSCAN settings: minPts = " + std::to_string(minPts));
		
		
	}
	
	//************************************************************************************************************************
	
	void use_only_hamming()
	{
		pure_hamming_approach = true;
		BenchmarkSingleton::Instance().Message("DBSCAN settings: using Hamming distance measure instead of Levenshtein distance");
	}
		
	void set_wish_for_tree_evaluation()
	{
		evaluate_trees_wished = true;		
	}	
		
	//************************************************************************************************************************
	
	void reduce_trees_to_roots()
	{
		// this calls a kernel function which makes the root vector point at the final roots
		unsigned int n_points = DataManager.get_n_entries();
		
		unsigned int blocks = (n_points < max_blocks[0])? n_points : (max_blocks[0]);
		blocks = ((blocks + max_threads - 1)/max_threads) * max_threads;
				
		tree_reduction_kernel<<<blocks, max_threads>>>(roots.begin(), n_points);
		
	}
	
	
	void evaluate_trees()
	{
		// this calls a kernel function which makes the root vector point at the final roots
		unsigned int n_points = DataManager.get_n_entries();
		
		unsigned int blocks = (n_points < max_blocks[0])? n_points : (max_blocks[0]);
		blocks = ((blocks + max_threads - 1)/max_threads) * max_threads;
		
		// 
		unsigned int maxdepth = 50;
		thrust::device_vector<unsigned int> depth_vector(maxdepth+1,0);
		
				
		tree_depth_evaluation_kernel<<<blocks, max_threads>>>(roots.begin(), depth_vector.begin(), n_points, maxdepth);
		
		BenchmarkSingleton::Instance().Message("Union find tree depth structure");
		for (unsigned int i = 0; i <= maxdepth; ++i)
		{	
			BenchmarkSingleton::Instance().Message("Number of trees with depth " + std::to_string(i) + ": " + std::to_string(depth_vector[i]));
		}
	}
	
	
	//************************************************************************************************************************
	// I found that searching the data for duplicates on the host is quite tedious and can be done in parallel the
	// same way as clustering occurs. This is actually a clustering, but only on exact duplicates. The advantage here is
	// that we can use a more efficient kernel. 
	// In the second clustering step, duplicates are inactivated
	
	void identify_duplicates()
	{
		// alternate version
	    unsigned int n_points = DataManager.get_n_entries();
	    
	    unsigned int ythreads = max_threads / 32;
	    ythreads = (ythreads > max_threads_dim[1])? max_threads_dim[1] : ythreads;
	    unsigned int xthreads = max_threads / ythreads;
	    ythreads = (xthreads > max_threads_dim[0])? max_threads_dim[0] : xthreads;
	    
		unsigned int xblocks = (n_points + xthreads - 1)/xthreads;
		xblocks = (xblocks < max_blocks[0])? xblocks : (max_blocks[0]);
				
		unsigned int yblocks = (n_points + ythreads - 1)/ythreads;
		yblocks = (yblocks < max_blocks[1])? yblocks : (max_blocks[1]);
		
		dim3 block_layout(xthreads, ythreads, 1);
		dim3 grid_layout(xblocks, yblocks, 1);
		
		BenchmarkSingleton::Instance().Message("Dispatching Kernel for finding duplicates with a " + std::to_string(xblocks) + "x" + std::to_string(yblocks) + 
			" grid that holds " + std::to_string(xthreads) + "x" + std::to_string(ythreads) + " threads");
	
		singlet_find_duplicates_kernel<<<grid_layout, block_layout>>>(DataManager.get_bitset_begin(), DataManager.get_bitset_properties(), DataManager.get_StringDataIterator(), roots.begin(), n_points);
	}
	
	void prepare_copies_vector()
	{
		// now, prepare the copies_counter
		thrust::device_vector<unsigned int> sorted_vals(roots.begin(), roots.end());
		thrust::device_vector<unsigned int> sum_vector(DataManager.get_n_entries());
		thrust::device_vector<unsigned int> keys_vector(DataManager.get_n_entries());
		
		thrust::sort(thrust::device, sorted_vals.begin(), sorted_vals.end(), thrust::greater<unsigned int>());
		auto end_iter = thrust::reduce_by_key(thrust::device, sorted_vals.begin(), sorted_vals.end(), thrust::make_constant_iterator(1u), keys_vector.begin(), sum_vector.begin());
		thrust::fill(thrust::device, copies.begin(), copies.end(), 0);  
		thrust::scatter(thrust::device, sum_vector.begin(), end_iter.second, keys_vector.begin(), copies.begin());
	}
	
	
	//************************************************************************************************************************
	
	
	
	
	//************************************************************************************************************************
	// This function identifies core points and non-core points. To reduce workload and expose as much parallelism as
	// possible, it somehow uses Pareto's law ... 80% of work can be done with 20% of effort.
		
	void identify_core_points()
	{
		thrust::copy(thrust::device,  copies.begin(), copies.end(), is_core_point_vector.begin());
		
		// and fill it with indices in increasing order
		thrust::sequence(thrust::device, uniques.begin(), uniques.end());
		
		// define an iterator that provides information whether an index points to itself in roots
		auto is_duplicate_iterator = thrust::make_transform_iterator(copies.begin(), equal_to_value_op<unsigned int, bool>(0u));
		
		// now, remove the duplicates
		auto end_indices_iterator = thrust::remove_if(thrust::device, uniques.begin(), uniques.end(), is_duplicate_iterator, thrust::identity<bool>());
		
		// transfer to a vector of this class
		n_unique = end_indices_iterator - uniques.begin();
		
		BenchmarkSingleton::Instance().Message("Number of uniques = " + std::to_string(n_unique));
		
		unsigned int xblocks = (n_unique < max_blocks[0])? n_unique : (max_blocks[0]);
		unsigned int yblocks = (n_unique < max_blocks[1])? n_unique : (max_blocks[1]);
		dim3 grid_layout(xblocks, yblocks, 1);
				
		epsilonOneFunctorClass<false> distanceFunctor(threshold);
		unsigned int stringlength = 1 + (1 + DataManager.get_max_strlen()) / 32;
		stringlength *= 32;

	    unsigned int nthreads = ((max_threads_by_multiprocessor / max_threads) > max_threads)? (max_threads_by_multiprocessor / max_threads) : max_threads;
		nthreads = (nthreads > max_threads)? max_threads : nthreads;
		nthreads = (nthreads > stringlength)? stringlength : nthreads;
		
		std::size_t shared_memory_usage = 2*sizeof(char)*stringlength + 2*sizeof(int) * ( 2 + stringlength);
			
			
		if (pure_hamming_approach)
		{	
			unsigned int ythreads = max_threads / 32;
			ythreads = (ythreads > max_threads_dim[1])? max_threads_dim[1] : ythreads;
			unsigned int xthreads = max_threads / ythreads;
			xthreads = (xthreads > max_threads_dim[0])? max_threads_dim[0] : xthreads;
	    
			xblocks = (n_unique + xthreads - 1)/xthreads;
			xblocks = (xblocks < max_blocks[0])? xblocks : (max_blocks[0]);
				
			yblocks = (n_unique + ythreads - 1)/ythreads;
			yblocks = (yblocks < max_blocks[1])? yblocks : (max_blocks[1]);
		
			dim3 ham_block_layout(xthreads, ythreads, 1);
			dim3 ham_grid_layout(xblocks, yblocks, 1);
							
			BenchmarkSingleton::Instance().Message("Dispatching Kernel for finding cores with a " + std::to_string(xblocks) + "x" + std::to_string(yblocks) + 
			" grid that holds " + std::to_string(xthreads) + "x" + std::to_string(ythreads) + " threads");
			
			half_hamming_core_points<<<ham_grid_layout, ham_block_layout>>>( 
				  DataManager.get_bitset_begin(), 
				  DataManager.get_bitset_properties(), 
				  DataManager.get_StringDataIterator(),
				  uniques.begin(),
				  copies.begin(),
				  is_core_point_vector.begin(),
				  n_unique,
				  stringlength,
				  distanceFunctor,
				  minPts);
		} else 
		{
			
			BenchmarkSingleton::Instance().Message("Dispatching Kernel for finding cores with a " + std::to_string(xblocks) + "x" + std::to_string(yblocks) + 
			" grid that holds one dimension of " + std::to_string(nthreads) + " threads");
	
			
			half_core_points<<<grid_layout, nthreads, shared_memory_usage>>>( 
					  DataManager.get_bitset_begin(), 
					  DataManager.get_bitset_properties(), 
					  DataManager.get_StringDataIterator(),
					  uniques.begin(),
					  copies.begin(),
					  is_core_point_vector.begin(),
					  n_unique,
					  stringlength,
					  distanceFunctor,
					  minPts);
		}
		
		thrust::transform(thrust::device, is_core_point_vector.begin(), is_core_point_vector.end(), is_core_point_vector.begin(), larger_or_equal_to_value_op<int,int>(minPts));
		
		int ncores = thrust::reduce(thrust::device, is_core_point_vector.begin(), is_core_point_vector.end());	
				  
	}
		
		
	//************************************************************************************************************************
		
	void grow_clusters()
	{
		thrust::device_vector<unsigned int> core_point_indices(n_unique);
		auto index_iter = thrust::make_counting_iterator(0u);
		auto ende_core_point_indices_iter = 
			thrust::copy_if(thrust::device, index_iter, index_iter + DataManager.get_n_entries(), is_core_point_vector.begin(), core_point_indices.begin(), thrust::identity<int>());
		core_point_indices.erase(ende_core_point_indices_iter, core_point_indices.end());	
		
		unsigned n_core_points = core_point_indices.size();
		BenchmarkSingleton::Instance().Message("Number of core points = " + std::to_string(n_core_points));
						
		unsigned int xblocks = (n_core_points < max_blocks[0])? n_core_points : (max_blocks[0]);
		unsigned int yblocks = (n_unique < max_blocks[1])? n_unique : (max_blocks[1]);
		dim3 grid_layout(xblocks, yblocks, 1);
				
		epsilonOneFunctorClass<false> distanceFunctor(threshold);
		unsigned int stringlength = 1 + (1 + DataManager.get_max_strlen()) / 32;
		stringlength *= 32;

	    unsigned int nthreads = ((max_threads_by_multiprocessor / max_threads) > max_threads)? (max_threads_by_multiprocessor / max_threads) : max_threads;
		nthreads = (nthreads > max_threads)? max_threads : nthreads;
		nthreads = (nthreads > stringlength)? stringlength : nthreads;

		std::size_t shared_memory_usage = 2*sizeof(char)*stringlength + 2*sizeof(int) * ( 2 + stringlength);
		
		if (pure_hamming_approach)
		{
			
			unsigned int ythreads = max_threads / 32;
			ythreads = (ythreads > max_threads_dim[1])? max_threads_dim[1] : ythreads;
			unsigned int xthreads = max_threads / ythreads;
			xthreads = (xthreads > max_threads_dim[0])? max_threads_dim[0] : xthreads;
	    
			xblocks = (n_core_points + xthreads - 1)/xthreads;
			xblocks = (xblocks < max_blocks[0])? xblocks : (max_blocks[0]);
				
			yblocks = (n_unique + ythreads - 1)/ythreads;
			yblocks = (yblocks < max_blocks[1])? yblocks : (max_blocks[1]);
		
			dim3 ham_block_layout(xthreads, ythreads, 1);
			dim3 ham_grid_layout(xblocks, yblocks, 1);
			
			fuse_hamming_clusters<<<ham_grid_layout, ham_block_layout>>>( 
						DataManager.get_bitset_begin(), 
						DataManager.get_bitset_properties(), 
					    DataManager.get_StringDataIterator(),
					    roots.begin(),
					    is_core_point_vector.begin(),
					    core_point_indices.begin(),
					    uniques.begin(),
					    n_core_points,
					    n_unique,
					    stringlength,
					    distanceFunctor);
		} else
		{
			fuse_clusters<<<grid_layout, nthreads, shared_memory_usage>>>( 
							DataManager.get_bitset_begin(), 
							DataManager.get_bitset_properties(), 
						    DataManager.get_StringDataIterator(),
						    roots.begin(),
						    is_core_point_vector.begin(),
						    core_point_indices.begin(),
						    uniques.begin(),
						    n_core_points,
						    n_unique,
						    stringlength,
						    distanceFunctor);
		}
		
		
	}	
	
	
	void re_number_clusters()
	{
		thrust::device_vector<unsigned int> intermediate(roots.begin(), roots.end());
		
				
		intermediate.erase(thrust::remove_if(thrust::device, intermediate.begin(), intermediate.end(), is_core_point_vector.begin(), thrust::logical_not<int>()), intermediate.end());
		
		
		thrust::fill(thrust::device, cluster_ids.begin(), cluster_ids.end(), 0);
		
		
		thrust::sort(thrust::device, intermediate.begin(), intermediate.end(), thrust::less<unsigned int>());
		
		
		intermediate.erase(thrust::unique(thrust::device, intermediate.begin(), intermediate.end()), intermediate.end());
			
		BenchmarkSingleton::Instance().Message("Total number of clusters = " + std::to_string(intermediate.size()));
		
		thrust::device_vector<unsigned int> intermediate3(roots.size(), 0);
		auto counter_iter = thrust::make_counting_iterator(1u);
		
		thrust::scatter(thrust::device, counter_iter, counter_iter + intermediate.size(), intermediate.begin(), intermediate3.begin());
		
		thrust::gather(thrust::device, roots.begin(), roots.end(), intermediate3.begin(), cluster_ids.begin());
		
		// determine number of noise points
		unsigned int n_noise = thrust::count(thrust::device, cluster_ids.begin(), cluster_ids.end(), 0);
		BenchmarkSingleton::Instance().Message("Number of noise points = " + std::to_string(n_noise));
		unsigned int perc_noise = (100*n_noise)/DataManager.get_n_entries();
		BenchmarkSingleton::Instance().Message("Percentage of noise points = " + std::to_string(perc_noise) + "%");	
		
	}
	
	
	//************************************************************************************************************************
	
	
	void doScan() {
		BenchmarkSingleton::Instance().StartTimer("Clustering core process");
		
		// prepare roots
		thrust::sequence(thrust::device, roots.begin(), roots.end());   // every entry directs to itself
		BenchmarkSingleton::Instance().Message("Total number of sequences = " + std::to_string(DataManager.get_n_entries()));
				
		// inactivate duplicates in a first step
		BenchmarkSingleton::Instance().StartTimer("Identify Duplicates");
		identify_duplicates();
		cudaDeviceSynchronize();
		BenchmarkSingleton::Instance().StopTimer("Identify Duplicates");
		
		BenchmarkSingleton::Instance().StartTimer("Reduction of duplicate information tree structure");
		reduce_trees_to_roots();
		cudaDeviceSynchronize();
		BenchmarkSingleton::Instance().StopTimer("Reduction of duplicate information tree structure");
		
		BenchmarkSingleton::Instance().StartTimer("Determine copies");
		prepare_copies_vector();
		cudaDeviceSynchronize();
		BenchmarkSingleton::Instance().StopTimer("Determine copies");
		
				
		// prepare core point indicator vector
		BenchmarkSingleton::Instance().StartTimer("Identify core points");
		thrust::fill(thrust::device, is_core_point_vector.begin(), is_core_point_vector.end(), 0);
		identify_core_points();
		cudaDeviceSynchronize();
		BenchmarkSingleton::Instance().StopTimer("Identify core points");
		
		
		// grow clusters
		BenchmarkSingleton::Instance().StartTimer("Grow clusters");
		grow_clusters();
		cudaDeviceSynchronize();
		BenchmarkSingleton::Instance().StopTimer("Grow clusters");
		
		if (evaluate_trees_wished) {
			
			evaluate_trees();	
		}
		
				
		BenchmarkSingleton::Instance().StartTimer("Reduction of cluster information tree structure");
		reduce_trees_to_roots();
		cudaDeviceSynchronize();
		BenchmarkSingleton::Instance().StopTimer("Reduction of cluster information tree structure");
		
		BenchmarkSingleton::Instance().StartTimer("Re-number clusters and identify noise");
		re_number_clusters();
		cudaDeviceSynchronize();
		BenchmarkSingleton::Instance().StopTimer("Re-number clusters and identify noise");
		
		BenchmarkSingleton::Instance().StopTimer("Clustering core process");
		
	}
		
	thrust::device_vector<unsigned int>::iterator get_cluster_ids()
	{
		// copy(thrust::device, roots.begin(), roots.end(), cluster_ids.begin());   // delete later
		return cluster_ids.begin();
	}
	
	thrust::device_vector<int>::iterator get_cores()
	{
		return is_core_point_vector.begin();
	}
	
	thrust::device_vector<int>::iterator get_copies_iterator()
	{
		return copies.begin();
	}
	
	thrust::device_vector<unsigned int>::iterator get_uniques_iterator()
	{
		return uniques.begin();
	}
	
	unsigned int get_n_uniques()
	{
		return n_unique;
	}
};
